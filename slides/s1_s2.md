# Plantilla de Presentación — Semanas 1 y 2 (Máx. 5 slides)

> **Regla:** máximo 5 “slides” (secciones).  
> **Tiempo total presentación:** 8-10 minutos.  

---

## Slide 1 — Semana 1 (Pregunta 1 + Respuesta 1)
**Pregunta 1:** ¿Qué tipo de evidencia de pruebas reduce incertidumbre sobre calidad sin confundir “testing” con “quality assurance”?

**Respuesta 1:**

<p align="justify">
El grupo coincide en que la reducción de la incertidumbre sobre la calidad del software requiere ir más allá de la verificación binaria de “pasa/falla” y apoyarse en evidencia empírica, cuantitativa, reproducible y trazable, obtenida principalmente mediante la ejecución directa de pruebas. Existe acuerdo en que el testing proporciona señales técnicas sobre el comportamiento del sistema bajo condiciones definidas a través de métricas como latencia, códigos de respuesta, cobertura, defect leakage, flakiness y pruebas funcionales y no funcionales, lo que permite estimar el riesgo residual, aunque sin garantizar ausencia total de defectos ni desempeño en producción. Asimismo, se reconoce que el quality assurance cumple un rol complementario al establecer marcos de proceso, prevención y coherencia, pero no sustituye la evidencia empírica del testing. En conjunto, el consenso establece que la calidad debe formularse como escenarios medibles y falsables (estímulo, entorno, respuesta, medida), con métricas claras, evidencia explícita y notas de validez del entorno, integrando testing como fuente empírica de datos y QA como soporte del proceso, sin confundir sus roles.
</p>

**Evidencias planeadas (cada uno con oráculo + archivo):**
- Contrato accesible: API REST operativa
    - Oráculo: HTTP 200 en endpoint base / documentación
    - Evidencia: Respuesta capturada mediante script de prueba -> evidence/openapi_response.json
- Operación básica: endpoint principal responde
    - Oráculo: HTTP 200 + JSON bien formado
    - Evidencia: Respuesta almacenada en -> evidence/basic_endpoint_response.json 
- Manejo mínimo de error: endpoint inexistente
    - Oráculo: HTTP 404
    - Evidencia: Registro del resultado en la salida del script de pruebas -> scripts/endpoint_test.sh
- Tiempo preliminar local (baseline)
    - Oráculo: Registro del time_total por ejecución.
    - Evidencia: Archivo generado por script de medición -> evidence/latency_summary.txt
      
**Límite :**  
“Esta evidencia no prueba seguridad, escalabilidad ni ausencia de defectos; únicamente reduce la incertidumbre sobre la disponibilidad, operación mínima y comportamiento básico de la API en un entorno local.”

---

## Slide 2 — Semana 2 (Pregunta 2 + Respuesta 2)
**Pregunta 2:** ¿Cómo convertir “calidad” en afirmaciones falsables y medibles?

**Respuesta 2:**
<p align="justify">
Para convertir la noción abstracta de calidad en un parámetro técnico utilizable, es necesario reemplazar los adjetivos subjetivos por escenarios de atributos de calidad definidos mediante la estructura de estímulo, entorno, respuesta y medida. Este enfoque permite que los requisitos de software se formulen como afirmaciones falsables, es decir, hipótesis empíricas que pueden ser refutadas cuando no alcanzan un umbral métrico explícito respaldado por evidencia documentada en archivos de datos.
La incorporación de metodologías como arc42 y ATAM facilita la evaluación de la arquitectura a través de criterios de aceptación rigurosos, especialmente en aspectos críticos como el rendimiento, la robustez y la disponibilidad. No obstante, toda medición debe considerar su contexto de validez, ya que los resultados obtenidos en entornos locales no son directamente extrapolables a producción, lo que garantiza un análisis fundamentado en la objetividad técnica.
</p>

**Escenarios S2 (elige 2 “estrella”):**
- **Escenario A — Rendimiento de Endpoints CRUD (Performance local)**  
  - Estímulo: Simulan 30 peticiones concurrentes de lectura: GET `/api/v1/juegos` contra la API.  
  - Entorno: API desplegada en entorno de pruebas con: Base de datos MongoDB activa, Configuraciones estándar (dev/prod) y Testing ejecutado desde una máquina de pruebas
  - Respuesta: La API debe procesar las solicitudes sin fallos de error 5xx.  
  - Medida: Tiempo medio de respuesta por tipo de operación y Porcentaje de errores
  - Evidencia: `evidence/week2/performance_results.csv` + `evidence/week2/performance_summary.txt`  
  - Falsación: Si algún caso devuelve HTTP code en el rango de 500, el escenario queda refutado.
- **Escenario B — Seguridad de Autenticación y Autorización (Security)**  
  - Estímulo: Acceso sin token a rutas con protección POST `/api/v1/juegos`; Acceso con token incorrecto o expirado.; Acceso con token válido y con rol suficiente (si aplica).
  - Entorno: API en entorno de pruebas con JWT configurado y políticas de autorización activas.  
  - Respuesta: La API debe rechazar accesos no autorizados y permitir accesos válidos.  
  - Medida: Las solicitudes no autenticadas o no autorizadas retornan 401 o 403, según corresponda; Las solicitudes con JWT válido retornan 200 o 201 en las acciones permitidas.
  - Evidencia: `evidence/week2/security_results.csv` + `evidence/week2/security_summary.txt`  
  - Falsación: Si algún caso devuelve HTTP codes distintos a los que mencionan en las medidas, el escenario queda refutado.

**Mini-tabla (obligatoria):**
| Claim | Escenario | Métrica | Evidencia (archivo) | Oráculo (pass/fail) |
|---|---|---|---|---|
| Tiempo preliminar local | Q1 Rendimiento de Endpoints CRUD | Tiempo medio de respuesta por tipo de operación y % de errores | `evidence/week2/performance_results.csv` + `evidence/week2/performance_summary.txt` | pass si HTTP Status code != 500 |
| Control de acceso | Q2 Seguridad de Autenticación y Autorización | Solicitudes no autenticadas/no autorizadas retornan 401/403; Las solicitudes con JWT válido retornan 200/201 | `evidence/week2/security_results.csv` + `evidence/week2/security_summary.txt`  | pass si HTTP Status code = 401/403; o solicitudes con JWT válido = 200/201|

---

## Slide 3 — Método formalizado (¿cómo trabajamos para definir escenarios?.)
**Proceso aplicado:**
1) Definimos los subgrupos a analizar (Rendimiento, Seguridad, Integridad y Robustez). 
2) Los expresamos como escenarios (E–Entorno–R–Medida) en quality/scenarios.md. 
3) Definimos bitácoras mínimos (HTTP 200/404, JSON válido, HTTP != 200). 
4) Generamos evidencia reproducible con scripts y la versionamos en evidence/week1 y evidence/week2.

**Fuentes para definición de método:**
- ISTQB: oráculo y objetivos de prueba (criterio de pass/fail).
- SWE@Google: testing como reducción de riesgo y claridad de pruebas.
- arc42/ATAM: escenarios medibles y verificables.

---

## Slide 4 — Amenazas a la validez 
**Amenazas a la validez (mínimo 3) + mitigación futura:**
**Validez interna: Variabilidad del entorno de ejecución**  
  **Amenaza:** Las pruebas se ejecutan en un entorno local controlado (Docker Compose, recursos limitados, sin balanceador), lo que puede introducir variabilidad no controlada en métricas como latencia, throughput o consumo de recursos.  
  **Mitigación futura:** Estandarizar la infraestructura de pruebas (CI/CD, contenedores con recursos fijados), repetir experimentos múltiples veces y reportar promedios, desviación estándar y percentiles.

**Validez de constructo: Métricas que no representan completamente la “calidad”**  
  **Amenaza:** Las métricas utilizadas (tiempo de respuesta, códigos HTTP, manejo de errores) pueden no capturar de forma completa los atributos de calidad definidos (rendimiento, robustez, seguridad), generando una medición parcial del constructo “calidad del software”.  
  **Mitigación futura:** Alinear explícitamente cada métrica con su escenario de calidad (estímulo–entorno–respuesta–medida), validar métricas contra estándares (arc42, ATAM) y complementar con métricas adicionales (p. ej., tasa de errores, degradación bajo carga).

**Validez externa: Limitada generalización de los resultados**  
  **Amenaza:** Los resultados obtenidos para el SUT *Game Shop* y un conjunto específico de endpoints y datos de prueba pueden no generalizarse a otros sistemas, dominios o cargas reales de producción.  
  **Mitigación futura:** Replicar los escenarios en otros SUT similares, variar perfiles de carga y datos de entrada, y ejecutar pruebas en entornos más cercanos a producción.

---

## Slide 5 — Cierre (2 conclusiones)
**Evidencia más fuerte:**  
  Evidencia empírica reproducible generada mediante scripts automatizados de pruebas (rendimiento, robustez y validación de errores), versionada en archivos CSV/JSON y trazable a comandos ejecutables.  
  *(Reduce incertidumbre porque los resultados son medibles, repetibles y verificables por terceros bajo las mismas condiciones experimentales).*

**Límite más crítico:**  
  Ejecución de pruebas exclusivamente en un entorno local controlado y sobre un único SUT (*Game Shop*), con configuraciones de infraestructura y cargas limitadas.  
  *(Impide generalizar los resultados a escenarios de producción, otros dominios de negocio o arquitecturas con mayor complejidad y variabilidad).*

**Conclusión adicional:**  
  La definición explícita de escenarios de calidad falsables permitió convertir atributos abstractos de “calidad” en criterios observables y medibles.  
  *(Fortalece el alineamiento entre requisitos no funcionales, pruebas ejecutadas y evidencia recolectada).*

**Siguiente mejora concreta:**  
  Integrar los escenarios de calidad en un pipeline CI/CD y replicar las pruebas en múltiples entornos (staging/producción simulada), incorporando variaciones de carga y datos representativos, sin modificar aún la implementación del SUT.

---
